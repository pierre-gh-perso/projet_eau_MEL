# src/load/load_to_bq.py

import pandas as pd
import sys
import os
from google.cloud import bigquery
from google.api_core import exceptions
import gcsfs # Assurez-vous d'avoir 'pip install gcsfs'

# Importation des variables d'environnement de la configuration
try:
    from config import GCS_BUCKET_NAME, BQ_DATASET_ID, GCP_PROJECT_ID
except ImportError:
    # Si config.py n'est pas trouv√© (peu probable dans le workflow, mais bonne pratique)
    print("Erreur critique: Impossible d'importer les variables de configuration.")
    sys.exit(1)


def load_processed_data_to_bigquery(project_id: str, dataset_id: str, gcs_bucket: str):
    """
    Lit les 4 tables Parquet depuis GCS/processed et les charge dans BigQuery.
    """
    if not all([project_id, gcs_bucket, dataset_id]):
        print("Erreur: Les variables Project ID, Bucket Name ou Dataset ID sont manquantes.")
        sys.exit(1)

    # 1. Connexion et V√©rification du Dataset
    client = bigquery.Client(project=project_id)
    dataset_ref = client.dataset(dataset_id)
    print(f"üîÑ Connexion √† BigQuery r√©ussie. Projet : {project_id}")

    try:
        # Cr√©er le dataset s'il n'existe pas
        client.get_dataset(dataset_ref)
        print(f"   Dataset '{dataset_id}' existe d√©j√†.")
    except exceptions.NotFound:
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = "europe-west1"  # Utilisez la m√™me localisation que votre bucket
        client.create_dataset(dataset)
        print(f"   ‚úÖ Dataset '{dataset_id}' cr√©√©.")

    # Les 4 noms de tables √† charger
    table_names = ['prelevements', 'parametres', 'resultats_mesures', 'communes_udi']
    
    # 2. Chargement des tables
    for table_name in table_names:
        
        # Le chemin exact est connu et unique car l'√©tape de transformation √©crit
        # un seul fichier Parquet par nom de table dans le dossier 'processed'
        gcs_file_path = f"gs://{gcs_bucket}/processed/{table_name}.parquet"
        table_id = f"{project_id}.{dataset_id}.{table_name}"
        
        try:
            print(f"\nüîÑ Tentative de lecture de la table '{table_name}' depuis {gcs_file_path}")
            
            # --- Utilisation de BigQuery pour charger directement depuis GCS (M√©thode 1: Rapide et √âvolutive) ---
            # NOTE: C'est l'approche la plus performante pour charger de gros volumes.
            
            job_config = bigquery.LoadJobConfig(
                source_format=bigquery.SourceFormat.PARQUET,
                write_disposition="WRITE_TRUNCATE", # Remplace la table √† chaque ex√©cution
                # Sch√©ma: BigQuery d√©duit le sch√©ma depuis le fichier Parquet.
            )
            
            # D√©marrage du job de chargement
            load_job = client.load_table_from_uri(
                gcs_file_path, 
                table_id, 
                job_config=job_config
            )
            
            print(f"   -> Chargement BQ d√©marr√©. Job ID: {load_job.job_id}")
            load_job.result()  # Attend la fin du job
            
            # Affichage du r√©sultat
            print(f"   ‚úÖ Table {table_name} charg√©e. {load_job.output_rows} lignes √©crites.")

        except exceptions.NotFound as e:
            # L√®ve une exception si le fichier Parquet est introuvable sur GCS
            print(f"   ‚ùå √âchec: Fichier Parquet non trouv√© sur GCS : {e}")
            raise  # Arr√™te le pipeline si une table essentielle manque
        except Exception as e:
            print(f"   ‚ùå √âchec critique du chargement BQ pour {table_name}: {e}")
            raise # Arr√™te le pipeline


def main():
    """
    Point d'entr√©e principal du module de chargement.
    """
    # L'authentification est g√©r√©e par GitHub Actions, le client BQ utilise le contexte
    load_processed_data_to_bigquery(GCP_PROJECT_ID, BQ_DATASET_ID, GCS_BUCKET_NAME)

if __name__ == "__main__":
    main()