# src/load/load_to_bq.py

import pandas as pd
import sys
import os
import io
from google.cloud import bigquery
from google.api_core import exceptions
import gcsfs 
from typing import List, Dict


# Importation des variables d'environnement de la configuration
try:
    from config import GCS_BUCKET_NAME, BQ_DATASET_ID, GCP_PROJECT_ID
except ImportError:
    print("Erreur critique: Impossible d'importer les variables de configuration.")
    sys.exit(1)

# Initialisation du client BQ
client = bigquery.Client(project=GCP_PROJECT_ID)

# Cl√©s primaires des tables pour la d√©duplication (uniquement pour les tables APPEND)
# Pour une d√©duplication parfaite sur toutes les tables, la cl√© doit √™tre d√©finie ici.
# Pour l'√©tape APPEND, on se concentre sur la table de Faits.
TABLE_PRIMARY_KEYS: Dict[str, List[str]] = {
    'resultats_mesures': ['code_prelevement', 'code_parametre'] # Cl√© composite pour les mesures
    # Les dimensions (prelevements, parametres, communes_reseau) seront TRUNCATE (WRITE_TRUNCATE)
}

# --- NOUVELLE FONCTION : LECTURE GCS & D√âDUPLICATION ---

def load_parquet_and_deduplicate(gcs_file_path: str, table_name: str, primary_keys: List[str], bq_table_id: str) -> pd.DataFrame:
    """
    Lit un fichier Parquet depuis GCS en m√©moire et le d√©duplique
    en comparant les cl√©s primaires avec celles d√©j√† pr√©sentes dans BigQuery.
    """
    # 1. Lecture du Parquet dans Pandas (N√©cessaire pour la d√©duplication)
    try:
        fs = gcsfs.GCSFileSystem()
        with fs.open(gcs_file_path, 'rb') as f:
            df = pd.read_parquet(io.BytesIO(f.read()))
    except FileNotFoundError as e:
        print(f" ¬† ‚ùå Fichier GCS non trouv√© √† l'emplacement : {gcs_file_path}")
        raise e
    
    initial_count = len(df)
    print(f" ¬† -> {initial_count} lignes lues depuis GCS. D√©but de la v√©rification des doublons...")

    # 2. D√©duplication pour les tables en mode APPEND (Faits)
    if table_name in TABLE_PRIMARY_KEYS:
        # Cr√©er une colonne cl√© temporaire pour le merge/filtrage
        df['temp_key'] = df[primary_keys].astype(str).agg('_'.join, axis=1)

        # Lire les cl√©s existantes depuis BigQuery
        query = f"SELECT DISTINCT {', '.join(primary_keys)} FROM `{bq_table_id}`"
        try:
            existing_keys_df = client.query(query).to_dataframe()
            existing_keys_df['temp_key'] = existing_keys_df[primary_keys].astype(str).agg('_'.join, axis=1)
            
            # Filtrer : Garder les lignes dont la cl√© n'existe PAS dans BQ
            df_dedup = df[~df['temp_key'].isin(existing_keys_df['temp_key'])].drop(columns=['temp_key']).copy()
            
            duplicates_count = initial_count - len(df_dedup)
            
            if duplicates_count > 0:
                print(f" ¬† üóëÔ∏è {duplicates_count} lignes en doublon identifi√©es (cl√©s : {primary_keys}) et supprim√©es.")
            
            return df_dedup
            
        except exceptions.NotFound:
            print(f" ¬† ‚ö†Ô∏è Table BQ '{bq_table_id}' non trouv√©e (premi√®re ex√©cution ?). Toutes les lignes seront charg√©es.")
            return df.drop(columns=['temp_key']).copy()

    # 3. Pour les tables de Dimensions (mode TRUNCATE), la d√©duplication est implicite.
    return df


# --- FONCTION PRINCIPALE DE CHARGEMENT BIGQUERY (MODIFI√âE) ---

def load_processed_data_to_bigquery(project_id: str, dataset_id: str, gcs_bucket: str):
    """
    Lit les 4 tables Parquet depuis GCS/processed, les d√©duplique et les charge dans BigQuery.
    """
    if not all([project_id, gcs_bucket, dataset_id]):
        print("Erreur: Les variables Project ID, Bucket Name ou Dataset ID sont manquantes.")
        sys.exit(1)

    # 1. Connexion et V√©rification du Dataset (Inchang√©)
    dataset_ref = client.dataset(dataset_id)
    print(f"üîÑ Connexion √† BigQuery r√©ussie. Projet : {project_id}")

    try:
        client.get_dataset(dataset_ref)
        print(f" ¬† Dataset '{dataset_id}' existe d√©j√†.")
    except exceptions.NotFound:
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = "europe-west1"
        client.create_dataset(dataset)
        print(f" ¬† ‚úÖ Dataset '{dataset_id}' cr√©√©.")

    # Les 4 noms de tables √† charger
    table_names = ['prelevements', 'parametres', 'communes_reseau', 'resultats_mesures']
    
    # 2. Chargement des tables
    for table_name in table_names:
        
        gcs_file_path = f"gs://{gcs_bucket}/processed/{table_name}.parquet"
        table_id = f"{project_id}.{dataset_id}.{table_name}"
        
        # D√©terminer la disposition d'√©criture
        write_mode_object = bigquery.WriteDisposition.WRITE_APPEND if table_name in TABLE_PRIMARY_KEYS else bigquery.WriteDisposition.WRITE_TRUNCATE
        
        write_mode_str = str(write_mode_object) 
        
        print(f"\nüîÑ Traitement de la table '{table_name}' (Mode: {write_mode_str.split('_')[1]})")
        
        try:
            # A. LECTURE & D√âDUPLICATION
            df_to_load = load_parquet_and_deduplicate(
                gcs_file_path, 
                table_name, 
                TABLE_PRIMARY_KEYS.get(table_name, []),
                table_id
            )

            if df_to_load.empty:
                print(f" ¬† ‚ÑπÔ∏è Aucune nouvelle ligne √† charger pour la table {table_name}. Skip.")
                continue

            # B. CHARGEMENT DANS BIGQUERY
            job_config = bigquery.LoadJobConfig(
                write_disposition=write_mode_object, 
            )
            
            # load_table_from_dataframe
            load_job = client.load_table_from_dataframe(
                df_to_load, 
                table_id, 
                job_config=job_config
            )
            
            print(f" ¬† -> Chargement BQ d√©marr√©. Job ID: {load_job.job_id}")
            load_job.result()
            
            print(f" ¬† ‚úÖ Table {table_name} charg√©e. {load_job.output_rows} lignes √©crites.")

        except exceptions.NotFound:
            print(f" ¬† ‚ùå Erreur: Le fichier {gcs_file_path} est introuvable. V√©rifiez l'√©tape de transformation.")
            raise
        except Exception as e:
            print(f" ¬† ‚ùå √âchec critique du chargement BQ pour {table_name}: {e}")
            raise


def main():
    """
    Point d'entr√©e principal du module de chargement.
    """
    print("--- D√©marrage de l'√âtape 3: Chargement BigQuery ---")
    load_processed_data_to_bigquery(GCP_PROJECT_ID, BQ_DATASET_ID, GCS_BUCKET_NAME)
    print("--- Fin de l'√âtape 3: Chargement BigQuery termin√©. ---")

if __name__ == "__main__":
    main()