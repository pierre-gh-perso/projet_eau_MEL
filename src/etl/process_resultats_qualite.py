# src/etl/process_resultats_qualite.py

import pandas as pd
import os
import sys
from typing import Dict, Any, List
import gcsfs # Pour interagir avec GCS
from config import GCS_BUCKET_NAME 

# ----------------------------------------------------------------------
# Fonctions utilitaires GCS (R√©utilis√©es de process_data_liste_communes.py)
# ----------------------------------------------------------------------

def get_latest_gcs_file(bucket_name: str, prefix: str, folder: str = "raw") -> str:
    """
    Trouve le chemin GCS complet du fichier Parquet le plus r√©cent.
    """
    fs = gcsfs.GCSFileSystem()
    gcs_dir = f"{bucket_name}/{folder}/"
    
    # Liste tous les fichiers qui correspondent au pr√©fixe
    gcs_files = fs.ls(gcs_dir)
    target_files = [
        f for f in gcs_files 
        if f.startswith(f"{gcs_dir}{prefix}") and f.endswith('.parquet')
    ]
    
    if not target_files:
        raise FileNotFoundError(f"Aucun fichier avec le pr√©fixe '{prefix}' n'a √©t√© trouv√© dans le dossier 'gs://{gcs_dir}'.")
        
    # Trie par nom (bas√© sur l'horodatage) et prend le plus r√©cent
    target_files.sort(reverse=True)
    
    # Reconstruire le chemin GCS complet
    latest_gcs_path = f"gs://{target_files[0]}"
    return latest_gcs_path

def save_df_to_gcs(df: pd.DataFrame, bucket_name: str, table_name: str):
    """
    Sauvegarde un DataFrame en Parquet dans le dossier GCS/processed.
    """
    gcs_path = f"gs://{bucket_name}/processed/{table_name}.parquet"
    print(f"   -> Sauvegarde de {len(df)} lignes dans {gcs_path}")
    df.to_parquet(gcs_path, index=False, engine='pyarrow', compression='snappy')
    print(f"   ‚úÖ Table {table_name} sauvegard√©e.")

# ----------------------------------------------------------------------
# Logique de Transformation et Normalisation
# ----------------------------------------------------------------------

def transform_and_normalize_data(raw_df: pd.DataFrame, mel_communes_insee: List[str]) -> Dict[str, pd.DataFrame]:
    """
    Filtre, nettoie et normalise les donn√©es brutes en 4 DataFrames (tables).
    """
    print("   -> D√©but du nettoyage et de la normalisation...")

    # 1. Nettoyage et Pr√©paration
    # S'assurer que les codes communes sont au bon format (5 chiffres)
    raw_df['code_commune'] = raw_df['code_commune'].astype(str).str.zfill(5)
    
    # Filtrage des r√©sultats de qualit√© pour la MEL
    mel_df = raw_df[raw_df['code_commune'].isin(mel_communes_insee)].copy()
    print(f"   -> Enregistrements filtr√©s pour la MEL : {len(mel_df)}")
    
    if mel_df.empty:
        raise ValueError("Aucun r√©sultat de qualit√© trouv√© pour les communes de la MEL apr√®s filtrage.")

    # D√©finition des cl√©s primaires et √©trang√®res
    # Cl√© Primaire du pr√©l√®vement
    mel_df['code_prelevement'] = mel_df['code_prelevement'].astype(str)
    # Cl√© Primaire du r√©sultat (utilis√©e pour l'unicit√© des mesures dans la table des faits)
    mel_df['resultat_id'] = mel_df['code_prelevement'] + mel_df['code_parametre'].astype(str)
    
    # --- 2. Construction de la table PARAM√àTRES (Dimension) ---
    # R√©cup√©rer l'information unique sur chaque type de param√®tre mesur√©
    params_cols = [
        'code_parametre', 'libelle_parametre', 'code_type_parametre', 
        'libelle_type_parametre', 'code_parametre_se'
    ]
    df_parametres = mel_df[params_cols].drop_duplicates().reset_index(drop=True)
    
    # --- 3. Construction de la table PR√âL√àVEMENTS (Fait/Dimension) ---
    # R√©cup√©rer l'information unique sur chaque pr√©l√®vement
    prelevement_cols = [
        'code_prelevement', 'code_commune', 'date_prelevement', 'nom_uge', 
        'nom_distributeur', 'nom_moa', 'conclusion_conformite', 'conformite_limites_bact'
    ]
    df_prelevements = mel_df[prelevement_cols].drop_duplicates(subset=['code_prelevement']).reset_index(drop=True)
    
    # --- 4. Construction de la table R√âSULTATS_MESURES (Fait) ---
    # La table principale des faits (chaque mesure)
    mesures_cols = [
        'code_prelevement', 'code_parametre', 'resultat_numerique', 
        'resultat_alphanumerique', 'libelle_unite', 'limite_qualite_parametre'
    ]
    # On utilise resultat_id pour garantir l'unicit√© de la mesure si l'API renvoie des doublons
    df_mesures = mel_df[mesures_cols].drop_duplicates(subset=['code_prelevement', 'code_parametre']).reset_index(drop=True)


    return {
        'parametres': df_parametres,
        'prelevements': df_prelevements,
        'resultats_mesures': df_mesures,
    }

# ----------------------------------------------------------------------
# Orchestrateur Principal
# ----------------------------------------------------------------------

def main():
    """
    Orchestre le T de l'ETL : Lecture GCS, Transformation, √âcriture GCS.
    """
    if not GCS_BUCKET_NAME:
        print("‚ùå √âchec de l'√©tape de transformation: GCS_BUCKET_NAME n'est pas d√©fini.")
        sys.exit(1)

    # ------------------------------------------------------
    # 1. Lecture des Donn√©es D'ENTR√âE (GCS)
    # ------------------------------------------------------
    
    # a) Lire la liste des codes INSEE de la MEL (produite √† l'√©tape pr√©c√©dente)
    try:
        udi_mel_gcs_path = f"gs://{GCS_BUCKET_NAME}/processed/communes_mel_udi.parquet"
        print(f"\nüîÑ Lecture de la liste INSEE MEL depuis {udi_mel_gcs_path}")
        df_udi_mel = pd.read_parquet(udi_mel_gcs_path, columns=['code_commune'])
        mel_communes_insee = df_udi_mel['code_commune'].astype(str).unique().tolist()
        print(f"   ‚úÖ {len(mel_communes_insee)} codes INSEE de la MEL charg√©s.")
    except Exception as e:
        print(f"‚ùå √âchec de la lecture de la liste INSEE MEL : {e}. L'√©tape 2 s'arr√™te.")
        sys.exit(1)
        
    # b) Lire le dernier fichier de r√©sultats de qualit√© bruts
    try:
        latest_raw_gcs_path = get_latest_gcs_file(GCS_BUCKET_NAME, "qualite_eau", folder="raw")
        print(f"üîÑ Lecture du fichier de r√©sultats bruts le plus r√©cent : {latest_raw_gcs_path}")
        raw_df_qualite = pd.read_parquet(latest_raw_gcs_path)
        print(f"   ‚úÖ {len(raw_df_qualite)} enregistrements bruts de qualit√© charg√©s.")
    except Exception as e:
        print(f"‚ùå √âchec de la lecture du fichier de r√©sultats bruts : {e}.")
        sys.exit(1)


    # ------------------------------------------------------
    # 2. Transformation et Normalisation
    # ------------------------------------------------------
    try:
        tables_dict = transform_and_normalize_data(raw_df_qualite, mel_communes_insee)
        
        # Ajout de la table COMMUNES_UDI filtr√©e, qui a √©t√© produite √† l'√©tape pr√©c√©dente
        # On lit le fichier communes_mel_udi.parquet et on le charge comme table dimension
        df_communes_udi = pd.read_parquet(udi_mel_gcs_path)
        df_communes_udi = df_communes_udi.rename(columns={'code_commune_insee': 'code_commune'})
        tables_dict['communes_udi'] = df_communes_udi
        
        print("‚úÖ Normalisation termin√©e. 4 tables pr√™tes pour le chargement.")

    except Exception as e:
        print(f"‚ùå √âchec de la transformation/normalisation : {e}")
        sys.exit(1)
        
    
    # ------------------------------------------------------
    # 3. √âcriture des 4 tables de Sortie (GCS/processed)
    # ------------------------------------------------------
    print("\n--- √âcriture des 4 tables normalis√©es vers GCS/processed ---")
    
    for table_name, df in tables_dict.items():
        try:
            save_df_to_gcs(df, GCS_BUCKET_NAME, table_name)
        except Exception as e:
            print(f"‚ùå √âchec critique de l'√©criture de la table {table_name}: {e}")
            sys.exit(1)

    print("--- Fin de l'√âtape 2: Transformation termin√©e. ---")


if __name__ == "__main__":
    main_cloud_ready()